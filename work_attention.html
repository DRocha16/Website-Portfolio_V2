<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="description" name="description">
  <meta name="google" content="notranslate" />
  <meta content="Mashup templates have been developped by Orson.io team" name="author">

  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">

  <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-icon-180x180.png">
  <link href="./assets/favicon.ico" rel="icon">

  <title>Denis Rocha &#8226; LuongAttentionDTA</title>

<link href="./main.3f6952e4.css" rel="stylesheet"></head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
  <nav class="navbar  navbar-fixed-top navbar-default">
    <div class="container">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      <div class="collapse navbar-collapse" id="navbar-collapse">
        <ul class="nav navbar-nav ">
          <li><a href="./index.html" title="">01 : Home</a></li>
          <li><a href="./works.html" title="">02 : Works</a></li>
          <li><a href="./about.html" title="">03 : About me</a></li>
          <li><a href="./contact.html" title="">04 : Contact</a></li>
          <!--<li><a href="./components.html" title="">05 : Components</a></li>-->
        </ul>


          <ul class="nav navbar-nav navbar-right navbar-small visible-md visible-lg">
            <li><a href="./work_boneage.html" title="">001</a></li>
            <li><a href="./work_osseo.html" title="">002</a></li>
            <li><a href="./Work_attention.html" title="" class="active">003</a></li>
            <li><a href="./work_emg.html" title="">004</a></li>
            <li><a href="./work_pet.html" title="">005</a></li>
            <li><a href="./work_fusion.html" title="">006</a></li>
          </ul>


      </div>
    </div>
  </nav>
</header>
<div class="section-container">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <img src="./assets/projects/protein3.jpg" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">003 : LuongAttentionDTA </h1>
          </div>
          <p><u>Why deep-learning?</u> Because the discovery of new protein-drug interactions, that is critical for drug discovery, can only otherwise be done by laboratory research which is extremely expensive, inefficient and time-consuming.
          </p>
          <blockquote>
            <p>"Drug target interaction prediction plays a vital role in the drug discovery process which aims to identify new drug compounds for biological targets."</p>
            <small class="pull-right">doi.org/10.1016/j.jbi.2019.103159</small>
          </blockquote>

        </div>
        <h3>Integrating Luong Attention with existing Deep-Learning algorithms such as DeepDTA[1] and AttentionDTA[2] </h3>
      <p>
This was a project for a faculty's course on Bioinformatics. The goal of this work was to implement from scratch the AttentionDTA[2] model and expand on it using Luong Attention and make some other changes in the model, such as the filter size of the CNN layer...
</p><p>The problem was a regression problem, since given the protein sequence and the drug sequence as a SMILES string, one had to predict the binding affinity between the two, that is nothing more than a number. The datasets used were the KIBA and the DAVIS dataset.
</p><p>Since a big difference on this model comparing with the others was the Luong Attention, the model was named LuongAttentionDTA.
</p><p>This work was done in Python using Tensorflow.
</p><p>The code can be acessed in this github page:  <a href="https:/github.com/DRocha16/LuongAttentionDTA">github.com/DRocha16/LuongAttentionDTA</a>
</p>
      </div>

      <div class="col-md-8 col-md-offset-2 section-container-spacer">
        <div class="row">
          <div class="col-xs-12 col-md-6">
            <img src="./assets/projects/model.jpg" class="img-responsive" alt="">
            <p> Model from AttentionDTA[2], which served as baseline. </p>
         </div>
         <div class="col-xs-12 col-md-6">
           <img src="./assets/projects/attention_model.png" class="img-responsive" alt="">
           <p> Schematic of the workings of a Luong Attention layer [3] </p>
        </div>

      </div>
    </div>


    <div class="col-md-8 col-md-offset-2 section-container-spacer">
      <h3> Here, lay some of the results of the model: </h3>
      <div class="row">
        <div class="col-xs-12 col-md-6">
          <img src="./assets/projects/preds_vs_trueDavis.jpg" class="img-responsive" alt="">
          <p> Predicted vs true values on Davis dataset. </p>
       </div>
       <div class="col-xs-12 col-md-6">
         <img src="./assets/projects/preds_vs_trueKIBA.jpg" class="img-responsive" alt="">
         <p> Predicted vs true values on KIBA dataset. </p>
      </div>
      <div class="col-xs-12 col-md-6">
        <img src="./assets/projects/tabela_davis.png" class="img-responsive" alt="">
        <p> Comparison of the results with other algoritms on the Davis dataset. </p>
     </div>
     <div class="col-xs-12 col-md-6">
       <img src="./assets/projects/tabela_kiba.png" class="img-responsive" alt="">
       <p> Comparison of the results with other algoritms on the KIBA dataset. </p>
    </div>

    <div class="col-xs-12 col-md-6">
      <img src="./assets/projects/attention_scores1.png" class="img-responsive" alt="">
      <p> Attention scores obtained from the model for a certain pair of drug-protein. </p>
   </div>
   <div class="col-xs-12 col-md-6">
     <img src="./assets/projects/attention_scores2.png" class="img-responsive" alt="">
     <p> Attention scores obtained from the model for another pair of drug-protein.. </p>
  </div>

    </div>
  </div>




    </div>
  </div>
</div>

<p>[1] - H. Öztürk, A. Özgür, and E. Ozkirimli, "DeepDTA: deep drug–target binding affinity prediction". Bioinformatics, vol. 34, no. 17, 821--829 (2018)
</p><p>[2] - Q. Zhao, F. Xiao, M. Yang, Y. Li, and J. Wang, "AttentionDTA: prediction of drug–target binding affinity using attention model". 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 64--69 (2019)
</p><p>[3] - https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a
</p>



<footer class="footer-container text-center">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <p>© 2021 | Website created with <a href="http://www.mashup-template.com/" title="Create website with free html template">Mashup Template</a>/<a href="https://www.unsplash.com/" title="Beautiful Free Images">Unsplash</a></p>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener("DOMContentLoaded", function (event) {
     navActivePage();
  });
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

--> <script type="text/javascript" src="./main.70a66962.js"></script></body>

</html>
